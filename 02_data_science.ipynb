{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Using Snowpark for Python and Auto Arima\n",
    "\n",
    "The purpose of this script is to demonstrate simple data science predictions on Snowflake objects using Snowpark for Python and Auto Arima. The intent is to begin with a Snowflake table containing monthly website sales data spanning multiple categories and create a predictive model to approximate future sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the various packages\n",
    "\n",
    "Before we can begin, we must import the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pmdarima\n",
    "import snowflake.snowpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InterWorks Snowpark package\n",
    "\n",
    "We must also import the required package from the InterWorks Snowpark package and leverage it to create a Snowflake Snowpark Session object that is connected to our Snowflake environment. Alternatively, you can modify the code to establish a Snowflake Snowpark Session through any method of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import module to build snowpark sessions\n",
    "from interworks_snowpark.snowpark_session_builder import build_snowpark_session_via_parameters_json as build_snowpark_session\n",
    "\n",
    "## Generate Snowpark session\n",
    "snowpark_session = build_snowpark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data\n",
    "\n",
    "Before we can train a model, we must retrieve the data that we wish to leverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variables that will be fed into the stored procedure\n",
    "\n",
    "By creating variables now, we can more easily convert our process to a Stored Procedure later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_table = 'WEBSITE_SALES'\n",
    "destination_table = 'WEBSITE_SALES_PREDICTIONS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the data from the source table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "|\"MONTH_OF_OPERATION\"  |\"CATEGORY\"  |\"SALES\"      |\n",
      "---------------------------------------------------\n",
      "|2020-06-01 00:00:00   |HIGH        |4667132.369  |\n",
      "|2020-07-01 00:00:00   |HIGH        |5537749.13   |\n",
      "|2020-08-01 00:00:00   |HIGH        |5539887.906  |\n",
      "|2020-09-01 00:00:00   |HIGH        |4905363.078  |\n",
      "|2020-10-01 00:00:00   |HIGH        |3318235.872  |\n",
      "|2020-10-01 00:00:00   |MEDIUM      |584250.14    |\n",
      "|2020-11-01 00:00:00   |HIGH        |2413273.809  |\n",
      "|2020-11-01 00:00:00   |MEDIUM      |1395640.868  |\n",
      "|2020-12-01 00:00:00   |HIGH        |1970506.003  |\n",
      "|2020-12-01 00:00:00   |MEDIUM      |1581726.646  |\n",
      "---------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_sf = snowpark_session.table(f'\"SALES_DB\".\"CLEAN\".\"{origin_table}\"') \n",
    "\n",
    "df_sales_sf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data into a Pandas dataframe\n",
    "\n",
    "Our current dataframe is a Snowflake dataframe, representing a query to an object in Snowflake. We wish to download this into a Pandas dataframe so that we can manipulate it more freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTH_OF_OPERATION</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>SALES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>389788.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>LOW</td>\n",
       "      <td>972043.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>2921744.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>361717.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>LOW</td>\n",
       "      <td>127406.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>3800767.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>210168.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>4750553.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>5411509.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1540465.694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MONTH_OF_OPERATION CATEGORY        SALES\n",
       "0           2017-01-01     HIGH   389788.900\n",
       "1           2017-01-01      LOW   972043.500\n",
       "2           2017-01-01   MEDIUM  2921744.500\n",
       "3           2017-02-01     HIGH   361717.200\n",
       "4           2017-02-01      LOW   127406.600\n",
       "..                 ...      ...          ...\n",
       "131         2022-05-01     HIGH  3800767.616\n",
       "132         2022-05-01   MEDIUM   210168.375\n",
       "133         2022-06-01     HIGH  4750553.049\n",
       "134         2022-07-01     HIGH  5411509.156\n",
       "135         2022-08-01     HIGH  1540465.694\n",
       "\n",
       "[136 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sales = pandas.DataFrame(data=df_sales_sf.collect()) \\\n",
    "  .sort_values(by=['MONTH_OF_OPERATION', 'CATEGORY'], ignore_index=True)\n",
    "\n",
    "display(df_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predictive model\n",
    "\n",
    "Now that we have our data, we are ready to begin constructing our predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Train\n",
    "\n",
    "Split our data into train and test, based on a predictive horizon of 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_periods = 24\n",
    "split_number = df_sales['SALES'].count() - pred_periods # corresponds to a prediction horizon of 2 years\n",
    "df_train     = pandas.DataFrame(df_sales['SALES'][:split_number]).rename(columns={'SALES':'y_train'})\n",
    "df_test      = pandas.DataFrame(df_sales['SALES'][split_number:]).rename(columns={'SALES':'y_test' })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Auto Arima model\n",
    "\n",
    "Leverage Auto Arima to create a model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,0,2)(1,0,1)[12] intercept   : AIC=3525.832, Time=0.48 sec\n",
      " ARIMA(0,0,0)(0,0,0)[12] intercept   : AIC=3545.533, Time=0.01 sec\n",
      " ARIMA(1,0,0)(1,0,0)[12] intercept   : AIC=3539.564, Time=0.06 sec\n",
      " ARIMA(0,0,1)(0,0,1)[12] intercept   : AIC=3541.961, Time=0.04 sec\n",
      " ARIMA(0,0,0)(0,0,0)[12]             : AIC=3648.498, Time=0.00 sec\n",
      " ARIMA(2,0,2)(0,0,1)[12] intercept   : AIC=3540.453, Time=0.12 sec\n",
      " ARIMA(2,0,2)(1,0,0)[12] intercept   : AIC=3540.386, Time=0.11 sec\n",
      " ARIMA(2,0,2)(2,0,1)[12] intercept   : AIC=3526.661, Time=1.01 sec\n",
      " ARIMA(2,0,2)(1,0,2)[12] intercept   : AIC=3525.846, Time=1.03 sec\n",
      " ARIMA(2,0,2)(0,0,0)[12] intercept   : AIC=3538.498, Time=0.04 sec\n",
      " ARIMA(2,0,2)(0,0,2)[12] intercept   : AIC=3534.069, Time=0.51 sec\n",
      " ARIMA(2,0,2)(2,0,0)[12] intercept   : AIC=3530.644, Time=0.41 sec\n",
      " ARIMA(2,0,2)(2,0,2)[12] intercept   : AIC=3524.210, Time=1.21 sec\n",
      " ARIMA(2,0,2)(3,0,2)[12] intercept   : AIC=3526.011, Time=2.51 sec\n",
      " ARIMA(2,0,2)(2,0,3)[12] intercept   : AIC=inf, Time=2.70 sec\n",
      " ARIMA(2,0,2)(1,0,3)[12] intercept   : AIC=3527.154, Time=2.45 sec\n",
      " ARIMA(2,0,2)(3,0,1)[12] intercept   : AIC=3529.094, Time=2.07 sec\n",
      " ARIMA(2,0,2)(3,0,3)[12] intercept   : AIC=3527.893, Time=3.02 sec\n",
      " ARIMA(1,0,2)(2,0,2)[12] intercept   : AIC=3525.480, Time=0.65 sec\n",
      " ARIMA(2,0,1)(2,0,2)[12] intercept   : AIC=3523.924, Time=0.68 sec\n",
      " ARIMA(2,0,1)(1,0,2)[12] intercept   : AIC=inf, Time=0.85 sec\n",
      " ARIMA(2,0,1)(2,0,1)[12] intercept   : AIC=3527.473, Time=0.41 sec\n",
      " ARIMA(2,0,1)(3,0,2)[12] intercept   : AIC=3525.842, Time=1.60 sec\n",
      " ARIMA(2,0,1)(2,0,3)[12] intercept   : AIC=3525.673, Time=2.05 sec\n",
      " ARIMA(2,0,1)(1,0,1)[12] intercept   : AIC=inf, Time=0.35 sec\n",
      " ARIMA(2,0,1)(1,0,3)[12] intercept   : AIC=inf, Time=2.03 sec\n",
      " ARIMA(2,0,1)(3,0,1)[12] intercept   : AIC=3528.857, Time=1.43 sec\n",
      " ARIMA(2,0,1)(3,0,3)[12] intercept   : AIC=3527.267, Time=2.32 sec\n",
      " ARIMA(1,0,1)(2,0,2)[12] intercept   : AIC=3523.968, Time=0.59 sec\n",
      " ARIMA(2,0,0)(2,0,2)[12] intercept   : AIC=3526.584, Time=0.45 sec\n",
      " ARIMA(3,0,1)(2,0,2)[12] intercept   : AIC=3524.903, Time=0.76 sec\n",
      " ARIMA(1,0,0)(2,0,2)[12] intercept   : AIC=3525.662, Time=0.39 sec\n",
      " ARIMA(3,0,0)(2,0,2)[12] intercept   : AIC=3524.985, Time=0.59 sec\n",
      " ARIMA(3,0,2)(2,0,2)[12] intercept   : AIC=3524.192, Time=1.31 sec\n",
      " ARIMA(2,0,1)(2,0,2)[12]             : AIC=inf, Time=0.89 sec\n",
      "\n",
      "Best model:  ARIMA(2,0,1)(2,0,2)[12] intercept\n",
      "Total fit time: 35.161 seconds\n"
     ]
    }
   ],
   "source": [
    "model_fit = pmdarima.auto_arima(df_train, test='adf', \n",
    "                         max_p=3, max_d=3, max_q=3, \n",
    "                         seasonal=True, m=12,\n",
    "                         max_P=3, max_D=2, max_Q=3,\n",
    "                         trace=True,\n",
    "                         error_action='ignore',  \n",
    "                         suppress_warnings=True, \n",
    "                         stepwise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarise model\n",
    "\n",
    "If desired, the model can be summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        SARIMAX Results                                        \n",
      "===============================================================================================\n",
      "Dep. Variable:                                       y   No. Observations:                  112\n",
      "Model:             SARIMAX(2, 0, 1)x(2, 0, [1, 2], 12)   Log Likelihood               -1752.962\n",
      "Date:                                 Wed, 07 Sep 2022   AIC                           3523.924\n",
      "Time:                                         16:38:36   BIC                           3548.390\n",
      "Sample:                                              0   HQIC                          3533.851\n",
      "                                                 - 112                                         \n",
      "Covariance Type:                                   opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept   1.104e+06   3.12e-08   3.54e+13      0.000     1.1e+06     1.1e+06\n",
      "ar.L1         -0.8758      0.299     -2.930      0.003      -1.462      -0.290\n",
      "ar.L2         -0.1951      0.156     -1.251      0.211      -0.501       0.111\n",
      "ma.L1          0.8080      0.270      2.993      0.003       0.279       1.337\n",
      "ar.S.L12      -0.0852      0.092     -0.925      0.355      -0.266       0.095\n",
      "ar.S.L24       0.8446      0.093      9.044      0.000       0.662       1.028\n",
      "ma.S.L12       0.1172      0.293      0.399      0.690      -0.458       0.692\n",
      "ma.S.L24      -0.4944      0.238     -2.074      0.038      -0.962      -0.027\n",
      "sigma2      2.793e+12   7.23e-14   3.86e+25      0.000    2.79e+12    2.79e+12\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.06   Jarque-Bera (JB):                 1.40\n",
      "Prob(Q):                              0.81   Prob(JB):                         0.50\n",
      "Heteroskedasticity (H):               0.56   Skew:                             0.19\n",
      "Prob(H) (two-sided):                  0.08   Kurtosis:                         2.61\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "[2] Covariance matrix is singular or near-singular, with condition number 1.35e+41. Standard errors may be unstable.\n"
     ]
    }
   ],
   "source": [
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate in-sample predictions\n",
    "\n",
    "The parameter `dynamic=False` means that the model makes predictions upon the lagged values. This means that the model is trained until a point in the time-series and then tries to predict the next value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predictions\n",
    "pred = model_fit.predict_in_sample(dynamic=False) # works only with auto-arima\n",
    "df_train['y_train_pred'] = pred\n",
    "\n",
    "# Calculate the percentage difference\n",
    "df_train['diff_percent'] = abs((df_train['y_train'] - pred) / df_train['y_train'])* 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions on test data\n",
    "\n",
    "Generate prediction for n periods. Predictions start from the last date of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model_fit.predict(n_periods=pred_periods, dynamic=False)\n",
    "df_test['y_test_pred'] = test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine test and train prediction values with original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTH_OF_OPERATION</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>SALES</th>\n",
       "      <th>TRAIN_PREDICTION</th>\n",
       "      <th>TEST_PREDICTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>389788.900</td>\n",
       "      <td>2.216705e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>LOW</td>\n",
       "      <td>972043.500</td>\n",
       "      <td>2.381818e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>2921744.500</td>\n",
       "      <td>2.571555e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>361717.200</td>\n",
       "      <td>2.117738e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>LOW</td>\n",
       "      <td>127406.600</td>\n",
       "      <td>2.308963e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>3800767.616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.134833e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>210168.375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.463570e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>4750553.049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.162073e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>5411509.156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.589970e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1540465.694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.656074e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MONTH_OF_OPERATION CATEGORY        SALES  TRAIN_PREDICTION  \\\n",
       "0           2017-01-01     HIGH   389788.900      2.216705e+06   \n",
       "1           2017-01-01      LOW   972043.500      2.381818e+06   \n",
       "2           2017-01-01   MEDIUM  2921744.500      2.571555e+06   \n",
       "3           2017-02-01     HIGH   361717.200      2.117738e+06   \n",
       "4           2017-02-01      LOW   127406.600      2.308963e+06   \n",
       "..                 ...      ...          ...               ...   \n",
       "131         2022-05-01     HIGH  3800767.616               NaN   \n",
       "132         2022-05-01   MEDIUM   210168.375               NaN   \n",
       "133         2022-06-01     HIGH  4750553.049               NaN   \n",
       "134         2022-07-01     HIGH  5411509.156               NaN   \n",
       "135         2022-08-01     HIGH  1540465.694               NaN   \n",
       "\n",
       "     TEST_PREDICTION  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  \n",
       "..               ...  \n",
       "131     3.134833e+06  \n",
       "132     1.463570e+06  \n",
       "133     4.162073e+06  \n",
       "134     4.589970e+06  \n",
       "135     4.656074e+06  \n",
       "\n",
       "[136 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_union = pandas.concat([df_sales, df_train, df_test], axis = 1) \\\n",
    "  .rename(columns={'y_train_pred':'TRAIN_PREDICTION', 'y_test_pred': 'TEST_PREDICTION'}) \\\n",
    "  [[\"MONTH_OF_OPERATION\", \"CATEGORY\", \"SALES\", \"TRAIN_PREDICTION\", \"TEST_PREDICTION\"]]\n",
    " \n",
    "display(df_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write output back to Snowflake\n",
    "\n",
    "Upload the data into the Snowflake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.table.Table at 0x185e9004820>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowpark_session.write_pandas(\n",
    "    df = df_union\n",
    "  , table_name = destination_table\n",
    "  , schema = 'MART'\n",
    "  , database = 'SALES_DB'\n",
    "  , auto_create_table = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Snowflake Stored Procedure\n",
    "\n",
    "Now that we have run through the above in steps, we can combine it all into a function and convert it into a stored procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function\n",
    "\n",
    "The first part of creating a Stored Procedure to deploy to Snowflake is to create the function that will become the Stored Procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_auto_arima_predictions(\n",
    "    snowpark_session: snowflake.snowpark.Session\n",
    "  , origin_table: str\n",
    "  , destination_table: str\n",
    ") :\n",
    "  # Retrieve the data from the source table\n",
    "  df_sales_sf = snowpark_session.table(f'\"SALES_DB\".\"CLEAN\".\"{origin_table}\"')\n",
    "\n",
    "  # Convert data into a Pandas dataframe\n",
    "  df_sales = pandas.DataFrame(data=df_sales_sf.collect()) \\\n",
    "    .sort_values(by=['MONTH_OF_OPERATION', 'CATEGORY'], ignore_index=True)\n",
    "\n",
    "  # Test and train\n",
    "  pred_periods = 24\n",
    "  split_number = df_sales['SALES'].count() - pred_periods # corresponds to a prediction horizon of 2 years\n",
    "  df_train     = pandas.DataFrame(df_sales['SALES'][:split_number]).rename(columns={'SALES':'y_train'})\n",
    "  df_test      = pandas.DataFrame(df_sales['SALES'][split_number:]).rename(columns={'SALES':'y_test' })\n",
    "\n",
    "  # Create Auto Arima model\n",
    "  model_fit = pmdarima.auto_arima(df_train, test='adf', \n",
    "                         max_p=3, max_d=3, max_q=3, \n",
    "                         seasonal=True, m=12,\n",
    "                         max_P=3, max_D=2, max_Q=3,\n",
    "                         trace=True,\n",
    "                         error_action='ignore',  \n",
    "                         suppress_warnings=True, \n",
    "                         stepwise=True)\n",
    "\n",
    "  # Generate in-sample predictions\n",
    "  pred = model_fit.predict_in_sample(dynamic=False) # works only with auto-arima\n",
    "  df_train['y_train_pred'] = pred\n",
    "\n",
    "  # Generate predictions on test data\n",
    "  test_pred = model_fit.predict(n_periods=pred_periods, dynamic=False)\n",
    "  df_test['y_test_pred'] = test_pred\n",
    "\n",
    "  # Combine test and train prediction values with original\n",
    "  df_union = pandas.concat([df_sales, df_train, df_test], axis = 1) \\\n",
    "    .rename(columns={'y_train_pred':'TRAIN_PREDICTION', 'y_test_pred': 'TEST_PREDICTION'}) \\\n",
    "    [[\"MONTH_OF_OPERATION\", \"CATEGORY\", \"SALES\", \"TRAIN_PREDICTION\", \"TEST_PREDICTION\"]]\n",
    "  \n",
    "  # Write output back to Snowflake\n",
    "  snowpark_session.write_pandas(\n",
    "      df = df_union\n",
    "    , table_name = destination_table\n",
    "    , schema = 'MART'\n",
    "    , database = 'SALES_DB'\n",
    "    , auto_create_table = True\n",
    "  )\n",
    "\n",
    "  return 'Complete'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import any required Snowpark objects\n",
    "\n",
    "Our stored procedure only requires the data type `StringType` as all inputs and outputs are strings. We must also import the function to create stored procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert function into Stored Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.stored_procedure.StoredProcedure at 0x185cf1a4490>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add required packages into the session creating our stored procedure \n",
    "snowpark_session.add_packages('snowflake-snowpark-python', 'pandas', 'pmdarima')\n",
    "\n",
    "# Upload SProc to Snowflake\n",
    "snowpark_session.sproc.register(\n",
    "    func = generate_auto_arima_predictions\n",
    "  , return_type = StringType()\n",
    "  , input_types = [StringType(), StringType()]\n",
    "  , is_permanent = True\n",
    "  , name = 'SALES_DB.PROCEDURES.GENERATE_AUTO_ARIMA_FUNCTION'\n",
    "  , replace = True\n",
    "  , stage_location = '@SALES_DB.PROCEDURES.MY_STAGE'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abca2f2ea4766feab0d5ed22ebf4aa677d5ebd9a43ee5e630cd7fa0b11fb4d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
