{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Using Snowpark for Python and Auto Arima\n",
    "\n",
    "The purpose of this script is to demonstrate simple data science predictions on Snowflake objects using Snowpark for Python and Auto Arima. The intent is to begin with a Snowflake table containing monthly website sales data spanning multiple categories and create a predictive model to approximate future sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the various packages\n",
    "\n",
    "Before we can begin, we must import the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pmdarima\n",
    "import snowflake.snowpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InterWorks Snowpark package\n",
    "\n",
    "We must also import the required package from the InterWorks Snowpark package and leverage it to create a Snowflake Snowpark Session object that is connected to our Snowflake environment. Alternatively, you can modify the code to establish a Snowflake Snowpark Session through any method of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import module to build snowpark sessions\n",
    "from interworks_snowpark.snowpark_session_builder import build_snowpark_session_via_parameters_json as build_snowpark_session\n",
    "\n",
    "## Generate Snowpark session\n",
    "snowpark_session = build_snowpark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data\n",
    "\n",
    "Before we can train a model, we must retrieve the data that we wish to leverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variables that will be fed into the stored procedure\n",
    "\n",
    "By creating variables now, we can more easily convert our process to a Stored Procedure later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_table = 'WEBSITE_SALES'\n",
    "destination_table = 'WEBSITE_SALES_PREDICTIONS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the data from the source table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "|\"MONTH_OF_OPERATION\"  |\"CATEGORY\"  |\"SALES\"      |\n",
      "---------------------------------------------------\n",
      "|2020-06-01 00:00:00   |HIGH        |4667132.369  |\n",
      "|2020-07-01 00:00:00   |HIGH        |5537749.13   |\n",
      "|2020-08-01 00:00:00   |HIGH        |5539887.906  |\n",
      "|2020-09-01 00:00:00   |HIGH        |4905363.078  |\n",
      "|2020-10-01 00:00:00   |HIGH        |3318235.872  |\n",
      "|2020-10-01 00:00:00   |MEDIUM      |584250.14    |\n",
      "|2020-11-01 00:00:00   |HIGH        |2413273.809  |\n",
      "|2020-11-01 00:00:00   |MEDIUM      |1395640.868  |\n",
      "|2020-12-01 00:00:00   |HIGH        |1970506.003  |\n",
      "|2020-12-01 00:00:00   |MEDIUM      |1581726.646  |\n",
      "---------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_sf = snowpark_session.table(f'\"SALES_DB\".\"CLEAN\".\"{origin_table}\"') \n",
    "\n",
    "df_sales_sf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data into a Pandas dataframe\n",
    "\n",
    "Our current dataframe is a Snowflake dataframe, representing a query to an object in Snowflake. We wish to download this into a Pandas dataframe so that we can manipulate it more freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTH_OF_OPERATION</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>SALES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>389788.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>LOW</td>\n",
       "      <td>972043.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>2921744.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>361717.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>LOW</td>\n",
       "      <td>127406.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>3800767.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>210168.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>4750553.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>5411509.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1540465.694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MONTH_OF_OPERATION CATEGORY        SALES\n",
       "0           2017-01-01     HIGH   389788.900\n",
       "1           2017-01-01      LOW   972043.500\n",
       "2           2017-01-01   MEDIUM  2921744.500\n",
       "3           2017-02-01     HIGH   361717.200\n",
       "4           2017-02-01      LOW   127406.600\n",
       "..                 ...      ...          ...\n",
       "131         2022-05-01     HIGH  3800767.616\n",
       "132         2022-05-01   MEDIUM   210168.375\n",
       "133         2022-06-01     HIGH  4750553.049\n",
       "134         2022-07-01     HIGH  5411509.156\n",
       "135         2022-08-01     HIGH  1540465.694\n",
       "\n",
       "[136 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sales = pandas.DataFrame(data=df_sales_sf.collect()) \\\n",
    "  .sort_values(by=['MONTH_OF_OPERATION', 'CATEGORY'], ignore_index=True)\n",
    "\n",
    "display(df_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predictive model\n",
    "\n",
    "Now that we have our data, we are ready to begin constructing our predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Train\n",
    "\n",
    "Split our data into train and test, based on a predictive horizon of 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_periods = 24\n",
    "split_number = df_sales['SALES'].count() - pred_periods # corresponds to a prediction horizon of 2 years\n",
    "df_train     = pandas.DataFrame(df_sales['SALES'][:split_number]).rename(columns={'SALES':'y_train'})\n",
    "df_test      = pandas.DataFrame(df_sales['SALES'][split_number:]).rename(columns={'SALES':'y_test' })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Auto Arima model\n",
    "\n",
    "Leverage Auto Arima to create a model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,0,2)(1,0,1)[12] intercept   : AIC=3525.832, Time=0.46 sec\n",
      " ARIMA(0,0,0)(0,0,0)[12] intercept   : AIC=3545.533, Time=0.01 sec\n",
      " ARIMA(1,0,0)(1,0,0)[12] intercept   : AIC=3539.564, Time=0.05 sec\n",
      " ARIMA(0,0,1)(0,0,1)[12] intercept   : AIC=3541.961, Time=0.04 sec\n",
      " ARIMA(0,0,0)(0,0,0)[12]             : AIC=3648.498, Time=0.00 sec\n",
      " ARIMA(2,0,2)(0,0,1)[12] intercept   : AIC=3540.453, Time=0.11 sec\n",
      " ARIMA(2,0,2)(1,0,0)[12] intercept   : AIC=3540.386, Time=0.11 sec\n",
      " ARIMA(2,0,2)(2,0,1)[12] intercept   : AIC=3526.661, Time=0.93 sec\n",
      " ARIMA(2,0,2)(1,0,2)[12] intercept   : AIC=3525.846, Time=0.97 sec\n",
      " ARIMA(2,0,2)(0,0,0)[12] intercept   : AIC=3538.498, Time=0.04 sec\n",
      " ARIMA(2,0,2)(0,0,2)[12] intercept   : AIC=3534.069, Time=0.48 sec\n",
      " ARIMA(2,0,2)(2,0,0)[12] intercept   : AIC=3530.644, Time=0.38 sec\n",
      " ARIMA(2,0,2)(2,0,2)[12] intercept   : AIC=3524.210, Time=1.14 sec\n",
      " ARIMA(2,0,2)(3,0,2)[12] intercept   : AIC=3526.011, Time=2.35 sec\n",
      " ARIMA(2,0,2)(2,0,3)[12] intercept   : AIC=inf, Time=2.53 sec\n",
      " ARIMA(2,0,2)(1,0,3)[12] intercept   : AIC=3527.154, Time=2.29 sec\n",
      " ARIMA(2,0,2)(3,0,1)[12] intercept   : AIC=3529.094, Time=1.97 sec\n",
      " ARIMA(2,0,2)(3,0,3)[12] intercept   : AIC=3527.893, Time=3.16 sec\n",
      " ARIMA(1,0,2)(2,0,2)[12] intercept   : AIC=3525.480, Time=0.66 sec\n",
      " ARIMA(2,0,1)(2,0,2)[12] intercept   : AIC=3523.924, Time=0.70 sec\n",
      " ARIMA(2,0,1)(1,0,2)[12] intercept   : AIC=inf, Time=0.84 sec\n",
      " ARIMA(2,0,1)(2,0,1)[12] intercept   : AIC=3527.473, Time=0.39 sec\n",
      " ARIMA(2,0,1)(3,0,2)[12] intercept   : AIC=3525.842, Time=1.54 sec\n",
      " ARIMA(2,0,1)(2,0,3)[12] intercept   : AIC=3525.673, Time=1.94 sec\n",
      " ARIMA(2,0,1)(1,0,1)[12] intercept   : AIC=inf, Time=0.35 sec\n",
      " ARIMA(2,0,1)(1,0,3)[12] intercept   : AIC=inf, Time=1.92 sec\n",
      " ARIMA(2,0,1)(3,0,1)[12] intercept   : AIC=3528.857, Time=1.38 sec\n",
      " ARIMA(2,0,1)(3,0,3)[12] intercept   : AIC=3527.267, Time=2.14 sec\n",
      " ARIMA(1,0,1)(2,0,2)[12] intercept   : AIC=3523.968, Time=0.55 sec\n",
      " ARIMA(2,0,0)(2,0,2)[12] intercept   : AIC=3526.584, Time=0.41 sec\n",
      " ARIMA(3,0,1)(2,0,2)[12] intercept   : AIC=3524.903, Time=0.73 sec\n",
      " ARIMA(1,0,0)(2,0,2)[12] intercept   : AIC=3525.662, Time=0.36 sec\n",
      " ARIMA(3,0,0)(2,0,2)[12] intercept   : AIC=3524.985, Time=0.55 sec\n",
      " ARIMA(3,0,2)(2,0,2)[12] intercept   : AIC=3524.192, Time=1.22 sec\n",
      " ARIMA(2,0,1)(2,0,2)[12]             : AIC=inf, Time=0.86 sec\n",
      "\n",
      "Best model:  ARIMA(2,0,1)(2,0,2)[12] intercept\n",
      "Total fit time: 33.564 seconds\n"
     ]
    }
   ],
   "source": [
    "model_fit = pmdarima.auto_arima(df_train, test='adf', \n",
    "                         max_p=3, max_d=3, max_q=3, \n",
    "                         seasonal=True, m=12,\n",
    "                         max_P=3, max_D=2, max_Q=3,\n",
    "                         trace=True,\n",
    "                         error_action='ignore',  \n",
    "                         suppress_warnings=True, \n",
    "                         stepwise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarise model\n",
    "\n",
    "If desired, the model can be summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        SARIMAX Results                                        \n",
      "===============================================================================================\n",
      "Dep. Variable:                                       y   No. Observations:                  112\n",
      "Model:             SARIMAX(2, 0, 1)x(2, 0, [1, 2], 12)   Log Likelihood               -1752.962\n",
      "Date:                                 Wed, 07 Sep 2022   AIC                           3523.924\n",
      "Time:                                         16:14:20   BIC                           3548.390\n",
      "Sample:                                              0   HQIC                          3533.851\n",
      "                                                 - 112                                         \n",
      "Covariance Type:                                   opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept   1.104e+06   3.12e-08   3.54e+13      0.000     1.1e+06     1.1e+06\n",
      "ar.L1         -0.8758      0.299     -2.930      0.003      -1.462      -0.290\n",
      "ar.L2         -0.1951      0.156     -1.251      0.211      -0.501       0.111\n",
      "ma.L1          0.8080      0.270      2.993      0.003       0.279       1.337\n",
      "ar.S.L12      -0.0852      0.092     -0.925      0.355      -0.266       0.095\n",
      "ar.S.L24       0.8446      0.093      9.044      0.000       0.662       1.028\n",
      "ma.S.L12       0.1172      0.293      0.399      0.690      -0.458       0.692\n",
      "ma.S.L24      -0.4944      0.238     -2.074      0.038      -0.962      -0.027\n",
      "sigma2      2.793e+12   7.23e-14   3.86e+25      0.000    2.79e+12    2.79e+12\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.06   Jarque-Bera (JB):                 1.40\n",
      "Prob(Q):                              0.81   Prob(JB):                         0.50\n",
      "Heteroskedasticity (H):               0.56   Skew:                             0.19\n",
      "Prob(H) (two-sided):                  0.08   Kurtosis:                         2.61\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "[2] Covariance matrix is singular or near-singular, with condition number 1.35e+41. Standard errors may be unstable.\n"
     ]
    }
   ],
   "source": [
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate in-sample predictions\n",
    "\n",
    "The parameter `dynamic=False` means that the model makes predictions upon the lagged values. This means that the model is trained until a point in the time-series and then tries to predict the next value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predictions\n",
    "pred = model_fit.predict_in_sample(dynamic=False) # works only with auto-arima\n",
    "df_train['y_train_pred'] = pred\n",
    "\n",
    "# Calculate the percentage difference\n",
    "df_train['diff_percent'] = abs((df_train['y_train'] - pred) / df_train['y_train'])* 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions on test data\n",
    "\n",
    "Generate prediction for n periods. Predictions start from the last date of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model_fit.predict(n_periods=pred_periods, dynamic=False)\n",
    "df_test['y_test_pred'] = test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine test and train prediction values with original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTH_OF_OPERATION</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>SALES</th>\n",
       "      <th>TRAIN_PREDICTION</th>\n",
       "      <th>TEST_PREDICTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>389788.900</td>\n",
       "      <td>2.216705e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>LOW</td>\n",
       "      <td>972043.500</td>\n",
       "      <td>2.381818e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>2921744.500</td>\n",
       "      <td>2.571555e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>361717.200</td>\n",
       "      <td>2.117738e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>LOW</td>\n",
       "      <td>127406.600</td>\n",
       "      <td>2.308963e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>3800767.616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.134833e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>210168.375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.463570e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>4750553.049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.162073e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>5411509.156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.589970e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1540465.694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.656074e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MONTH_OF_OPERATION CATEGORY        SALES  TRAIN_PREDICTION  \\\n",
       "0           2017-01-01     HIGH   389788.900      2.216705e+06   \n",
       "1           2017-01-01      LOW   972043.500      2.381818e+06   \n",
       "2           2017-01-01   MEDIUM  2921744.500      2.571555e+06   \n",
       "3           2017-02-01     HIGH   361717.200      2.117738e+06   \n",
       "4           2017-02-01      LOW   127406.600      2.308963e+06   \n",
       "..                 ...      ...          ...               ...   \n",
       "131         2022-05-01     HIGH  3800767.616               NaN   \n",
       "132         2022-05-01   MEDIUM   210168.375               NaN   \n",
       "133         2022-06-01     HIGH  4750553.049               NaN   \n",
       "134         2022-07-01     HIGH  5411509.156               NaN   \n",
       "135         2022-08-01     HIGH  1540465.694               NaN   \n",
       "\n",
       "     TEST_PREDICTION  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  \n",
       "..               ...  \n",
       "131     3.134833e+06  \n",
       "132     1.463570e+06  \n",
       "133     4.162073e+06  \n",
       "134     4.589970e+06  \n",
       "135     4.656074e+06  \n",
       "\n",
       "[136 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_union = pandas.concat([df_sales, df_train, df_test], axis = 1) \\\n",
    "  .rename(columns={'y_train_pred':'TRAIN_PREDICTION', 'y_test_pred': 'TEST_PREDICTION'}) \\\n",
    "  [[\"MONTH_OF_OPERATION\", \"CATEGORY\", \"SALES\", \"TRAIN_PREDICTION\", \"TEST_PREDICTION\"]]\n",
    " \n",
    "display(df_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write output back to Snowflake\n",
    "\n",
    "Upload the data into the Snowflake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.table.Table at 0x29d4b925790>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowpark_session.write_pandas(\n",
    "    df = df_union\n",
    "  , table_name = destination_table\n",
    "  , schema = 'MART'\n",
    "  , database = 'SALES_DB'\n",
    "  , auto_create_table = True\n",
    "  , overwrite = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Snowflake Stored Procedure\n",
    "\n",
    "Now that we have run through the above in steps, we can combine it all into a function and convert it into a stored procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function\n",
    "\n",
    "The first part of creating a Stored Procedure to deploy to Snowflake is to create the function that will become the Stored Procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_auto_arima_predictions(\n",
    "    snowpark_session: snowflake.snowpark.Session\n",
    "  , origin_table: str\n",
    "  , destination_table: str\n",
    ") :\n",
    "  # Retrieve the data from the source table\n",
    "  df_sales_sf = snowpark_session.table(f'\"SALES_DB\".\"CLEAN\".\"{origin_table}\"')\n",
    "\n",
    "  # Convert data into a Pandas dataframe\n",
    "  df_sales = pandas.DataFrame(data=df_sales_sf.collect()) \\\n",
    "    .sort_values(by=['MONTH_OF_OPERATION', 'CATEGORY'], ignore_index=True)\n",
    "\n",
    "  # Test and train\n",
    "  pred_periods = 24\n",
    "  split_number = df_sales['SALES'].count() - pred_periods # corresponds to a prediction horizon of 2 years\n",
    "  df_train     = pandas.DataFrame(df_sales['SALES'][:split_number]).rename(columns={'SALES':'y_train'})\n",
    "  df_test      = pandas.DataFrame(df_sales['SALES'][split_number:]).rename(columns={'SALES':'y_test' })\n",
    "\n",
    "  # Create Auto Arima model\n",
    "  model_fit = pmdarima.auto_arima(df_train, test='adf', \n",
    "                         max_p=3, max_d=3, max_q=3, \n",
    "                         seasonal=True, m=12,\n",
    "                         max_P=3, max_D=2, max_Q=3,\n",
    "                         trace=True,\n",
    "                         error_action='ignore',  \n",
    "                         suppress_warnings=True, \n",
    "                         stepwise=True)\n",
    "\n",
    "  # Generate in-sample predictions\n",
    "  pred = model_fit.predict_in_sample(dynamic=False) # works only with auto-arima\n",
    "  df_train['y_train_pred'] = pred\n",
    "\n",
    "  # Generate predictions on test data\n",
    "  test_pred = model_fit.predict(n_periods=pred_periods, dynamic=False)\n",
    "  df_test['y_test_pred'] = test_pred\n",
    "\n",
    "  # Combine test and train prediction values with original\n",
    "  df_union = pandas.concat([df_sales, df_train, df_test], axis = 1) \\\n",
    "    .rename(columns={'y_train_pred':'TRAIN_PREDICTION', 'y_test_pred': 'TEST_PREDICTION'}) \\\n",
    "    [[\"MONTH_OF_OPERATION\", \"CATEGORY\", \"SALES\", \"TRAIN_PREDICTION\", \"TEST_PREDICTION\"]]\n",
    "  \n",
    "  # Write output back to Snowflake\n",
    "  snowpark_session.write_pandas(\n",
    "      df = df_union\n",
    "    , table_name = destination_table\n",
    "    , schema = 'MART'\n",
    "    , database = 'SALES_DB'\n",
    "    , auto_create_table = True\n",
    "    , overwrite = True\n",
    "  )\n",
    "\n",
    "  return 'Complete'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import any required Snowpark objects\n",
    "\n",
    "Our stored procedure only requires the data type `StringType` as all inputs and outputs are strings. We must also import the function to create stored procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert function into Stored Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to execute query [queryID: 01a6cfe9-0501-d420-0071-650300055106] \n",
      "CREATE OR REPLACE \n",
      " PROCEDURE SALES_DB.PROCEDURES.GENERATE_AUTO_ARIMA_FUNCTION(arg1 STRING,arg2 STRING)\n",
      "RETURNS STRING\n",
      "LANGUAGE PYTHON\n",
      "RUNTIME_VERSION=3.8\n",
      "\n",
      "PACKAGES=('snowflake-snowpark-python[pandas]','pandas','pmdarima','cloudpickle==2.0.0')\n",
      "HANDLER='compute'\n",
      "\n",
      "AS $$\n",
      "import pickle\n",
      "\n",
      "func = pickle.loads(bytes.fromhex('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'))\n",
      "\n",
      "def compute(session,arg1,arg2):\n",
      "    return func(session,arg1,arg2)\n",
      "$$\n",
      "\n",
      "\n",
      "001422 (22023): SQL compilation error:\n",
      "invalid value 'snowflake-snowpark-python[pandas]' for property 'PACKAGES'\n"
     ]
    },
    {
     "ename": "SnowparkSQLException",
     "evalue": "(1304): 001422 (22023): SQL compilation error:\ninvalid value 'snowflake-snowpark-python[pandas]' for property 'PACKAGES'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [39], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m snowpark_session\u001b[38;5;241m.\u001b[39madd_packages(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnowflake-snowpark-python[pandas]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpmdarima\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Upload SProc to Snowflake\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43msnowpark_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgenerate_auto_arima_predictions\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mStringType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mStringType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStringType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_permanent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSALES_DB.PROCEDURES.GENERATE_AUTO_ARIMA_FUNCTION\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_location\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m@SALES_DB.PROCEDURES.MY_STAGE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\snowpark\\stored_procedure.py:382\u001b[0m, in \u001b[0;36mStoredProcedureRegistration.register\u001b[1;34m(self, func, return_type, input_types, name, is_permanent, stage_location, imports, packages, replace, parallel, statement_params)\u001b[0m\n\u001b[0;32m    377\u001b[0m check_register_args(\n\u001b[0;32m    378\u001b[0m     TempObjectType\u001b[39m.\u001b[39mPROCEDURE, name, is_permanent, stage_location, parallel\n\u001b[0;32m    379\u001b[0m )\n\u001b[0;32m    381\u001b[0m \u001b[39m# register stored procedure\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_register_sp(\n\u001b[0;32m    383\u001b[0m     func,\n\u001b[0;32m    384\u001b[0m     return_type,\n\u001b[0;32m    385\u001b[0m     input_types,\n\u001b[0;32m    386\u001b[0m     name,\n\u001b[0;32m    387\u001b[0m     stage_location,\n\u001b[0;32m    388\u001b[0m     imports,\n\u001b[0;32m    389\u001b[0m     packages,\n\u001b[0;32m    390\u001b[0m     replace,\n\u001b[0;32m    391\u001b[0m     parallel,\n\u001b[0;32m    392\u001b[0m     statement_params\u001b[39m=\u001b[39;49mstatement_params,\n\u001b[0;32m    393\u001b[0m )\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\snowpark\\stored_procedure.py:577\u001b[0m, in \u001b[0;36mStoredProcedureRegistration._do_register_sp\u001b[1;34m(self, func, return_type, input_types, sp_name, stage_location, imports, packages, replace, parallel, statement_params)\u001b[0m\n\u001b[0;32m    573\u001b[0m     tb \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m]\n\u001b[0;32m    574\u001b[0m     ne \u001b[39m=\u001b[39m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[0;32m    575\u001b[0m         pe\n\u001b[0;32m    576\u001b[0m     )\n\u001b[1;32m--> 577\u001b[0m     \u001b[39mraise\u001b[39;00m ne\u001b[39m.\u001b[39mwith_traceback(tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    578\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[0;32m    579\u001b[0m     raised \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\snowpark\\stored_procedure.py:554\u001b[0m, in \u001b[0;36mStoredProcedureRegistration._do_register_sp\u001b[1;34m(self, func, return_type, input_types, sp_name, stage_location, imports, packages, replace, parallel, statement_params)\u001b[0m\n\u001b[0;32m    552\u001b[0m raised \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m     create_python_udf_or_sp(\n\u001b[0;32m    555\u001b[0m         session\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session,\n\u001b[0;32m    556\u001b[0m         return_type\u001b[39m=\u001b[39;49mreturn_type,\n\u001b[0;32m    557\u001b[0m         input_args\u001b[39m=\u001b[39;49minput_args,\n\u001b[0;32m    558\u001b[0m         handler\u001b[39m=\u001b[39;49mhandler,\n\u001b[0;32m    559\u001b[0m         object_type\u001b[39m=\u001b[39;49mTempObjectType\u001b[39m.\u001b[39;49mPROCEDURE,\n\u001b[0;32m    560\u001b[0m         object_name\u001b[39m=\u001b[39;49mudf_name,\n\u001b[0;32m    561\u001b[0m         all_imports\u001b[39m=\u001b[39;49mall_imports,\n\u001b[0;32m    562\u001b[0m         all_packages\u001b[39m=\u001b[39;49mall_packages,\n\u001b[0;32m    563\u001b[0m         is_temporary\u001b[39m=\u001b[39;49mstage_location \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    564\u001b[0m         replace\u001b[39m=\u001b[39;49mreplace,\n\u001b[0;32m    565\u001b[0m         inline_python_code\u001b[39m=\u001b[39;49mcode,\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[39m# an exception might happen during registering a stored procedure\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[39m# (e.g., a dependency might not be found on the stage),\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[39m# then for a permanent stored procedure, we should delete the uploaded\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[39m# python file and raise the exception\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[39mexcept\u001b[39;00m ProgrammingError \u001b[39mas\u001b[39;00m pe:\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\udf_utils.py:672\u001b[0m, in \u001b[0;36mcreate_python_udf_or_sp\u001b[1;34m(session, return_type, input_args, handler, object_type, object_name, all_imports, all_packages, is_temporary, replace, inline_python_code)\u001b[0m\n\u001b[0;32m    651\u001b[0m     inline_python_code_in_sql \u001b[39m=\u001b[39m (\n\u001b[0;32m    652\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    653\u001b[0m \u001b[39mAS $$\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    659\u001b[0m     )\n\u001b[0;32m    661\u001b[0m     create_query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    662\u001b[0m \u001b[39mCREATE \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39mOR REPLACE \u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m replace \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m}\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39mTEMPORARY\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m is_temporary \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mobject_type\u001b[39m.\u001b[39mvalue\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mobject_name\u001b[39m}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00msql_func_args\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[39m{\u001b[39;00minline_python_code_in_sql\u001b[39m}\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m--> 672\u001b[0m     session\u001b[39m.\u001b[39;49m_run_query(create_query, is_ddl_on_temp_object\u001b[39m=\u001b[39;49mis_temporary)\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\snowpark\\session.py:951\u001b[0m, in \u001b[0;36mSession._run_query\u001b[1;34m(self, query, is_ddl_on_temp_object)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_query\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, is_ddl_on_temp_object: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Any]:\n\u001b[1;32m--> 951\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrun_query(query, is_ddl_on_temp_object\u001b[39m=\u001b[39;49mis_ddl_on_temp_object)[\n\u001b[0;32m    952\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:104\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m    101\u001b[0m         ex\u001b[39m.\u001b[39mcause\n\u001b[0;32m    102\u001b[0m     )\n\u001b[0;32m    103\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m--> 104\u001b[0m     \u001b[39mraise\u001b[39;00m ex\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:98\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[0;32m     97\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     99\u001b[0m \u001b[39mexcept\u001b[39;00m ReauthenticationRequest \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m    100\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m    101\u001b[0m         ex\u001b[39m.\u001b[39mcause\n\u001b[0;32m    102\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:333\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[1;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m     query_id_log \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m [queryID: \u001b[39m\u001b[39m{\u001b[39;00mex\u001b[39m.\u001b[39msfqid\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(ex, \u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to execute query\u001b[39m\u001b[39m{\u001b[39;00mquery_id_log\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mex\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 333\u001b[0m     \u001b[39mraise\u001b[39;00m ex\n\u001b[0;32m    335\u001b[0m \u001b[39m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[39m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[39m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[39m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m to_pandas:\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:325\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[1;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39m_statement_params\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {}\n\u001b[0;32m    324\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_statement_params\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mSNOWPARK_SKIP_TXN_COMMIT_IN_DDL\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m results_cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cursor\u001b[39m.\u001b[39;49mexecute(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    326\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify_query_listeners(\n\u001b[0;32m    327\u001b[0m     QueryRecord(results_cursor\u001b[39m.\u001b[39msfqid, results_cursor\u001b[39m.\u001b[39mquery)\n\u001b[0;32m    328\u001b[0m )\n\u001b[0;32m    329\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecute query [queryID: \u001b[39m\u001b[39m{\u001b[39;00mresults_cursor\u001b[39m.\u001b[39msfqid\u001b[39m}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\connector\\cursor.py:804\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[1;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, file_stream)\u001b[0m\n\u001b[0;32m    800\u001b[0m     is_integrity_error \u001b[39m=\u001b[39m (\n\u001b[0;32m    801\u001b[0m         code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m100072\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    802\u001b[0m     )  \u001b[39m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[0;32m    803\u001b[0m     error_class \u001b[39m=\u001b[39m IntegrityError \u001b[39mif\u001b[39;00m is_integrity_error \u001b[39melse\u001b[39;00m ProgrammingError\n\u001b[1;32m--> 804\u001b[0m     Error\u001b[39m.\u001b[39;49merrorhandler_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnection, \u001b[39mself\u001b[39;49m, error_class, errvalue)\n\u001b[0;32m    805\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\connector\\errors.py:276\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merrorhandler_wrapper\u001b[39m(\n\u001b[0;32m    255\u001b[0m     connection: SnowflakeConnection \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     error_value: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m \u001b[39mint\u001b[39m],\n\u001b[0;32m    259\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m     \u001b[39m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39m        exception to the first handler in that order.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     handed_over \u001b[39m=\u001b[39m Error\u001b[39m.\u001b[39;49mhand_to_other_handler(\n\u001b[0;32m    277\u001b[0m         connection,\n\u001b[0;32m    278\u001b[0m         cursor,\n\u001b[0;32m    279\u001b[0m         error_class,\n\u001b[0;32m    280\u001b[0m         error_value,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m handed_over:\n\u001b[0;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m Error\u001b[39m.\u001b[39merrorhandler_make_exception(\n\u001b[0;32m    284\u001b[0m             error_class,\n\u001b[0;32m    285\u001b[0m             error_value,\n\u001b[0;32m    286\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\connector\\errors.py:331\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m cursor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     cursor\u001b[39m.\u001b[39mmessages\u001b[39m.\u001b[39mappend((error_class, error_value))\n\u001b[1;32m--> 331\u001b[0m     cursor\u001b[39m.\u001b[39;49merrorhandler(connection, cursor, error_class, error_value)\n\u001b[0;32m    332\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39melif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Git Repositories\\BUILD 2022\\.venv\\lib\\site-packages\\snowflake\\connector\\errors.py:210\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_errorhandler\u001b[39m(\n\u001b[0;32m    194\u001b[0m     connection: SnowflakeConnection,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     error_value: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m],\n\u001b[0;32m    198\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39m\"\"\"Default error handler that raises an error.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39m        A Snowflake error.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(\n\u001b[0;32m    211\u001b[0m         msg\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmsg\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    212\u001b[0m         errno\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39merrno\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    213\u001b[0m         sqlstate\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msqlstate\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    214\u001b[0m         sfqid\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    215\u001b[0m         done_format_msg\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdone_format_msg\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    216\u001b[0m         connection\u001b[39m=\u001b[39mconnection,\n\u001b[0;32m    217\u001b[0m         cursor\u001b[39m=\u001b[39mcursor,\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mSnowparkSQLException\u001b[0m: (1304): 001422 (22023): SQL compilation error:\ninvalid value 'snowflake-snowpark-python[pandas]' for property 'PACKAGES'"
     ]
    }
   ],
   "source": [
    "# Add required packages into the session creating our stored procedure \n",
    "snowpark_session.add_packages('snowflake-snowpark-python', 'pandas', 'pmdarima')\n",
    "\n",
    "# Upload SProc to Snowflake\n",
    "snowpark_session.sproc.register(\n",
    "    func = generate_auto_arima_predictions\n",
    "  , return_type = StringType()\n",
    "  , input_types = [StringType(), StringType()]\n",
    "  , is_permanent = True\n",
    "  , name = 'SALES_DB.PROCEDURES.GENERATE_AUTO_ARIMA_FUNCTION'\n",
    "  , replace = True\n",
    "  , stage_location = '@SALES_DB.PROCEDURES.MY_STAGE'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abca2f2ea4766feab0d5ed22ebf4aa677d5ebd9a43ee5e630cd7fa0b11fb4d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
