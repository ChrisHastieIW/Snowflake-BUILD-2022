{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation Using Snowpark for Python\n",
    "\n",
    "The purpose of this script is to demonstrate simple data transformations on Snowflake objects using Snowpark for Python. The intent is to begin with a Snowflake table containing hourly website sales data spanning 15 years and perform the following transformations:\n",
    "\n",
    "- Filter the data to 2017 onwards and only when the website is operating normally\n",
    "- Categorise each hour based on the bounce rate for the website (i.e. the percentage of website viewers who did not put an item in the basket)\n",
    "- Aggregate the number of sales by month and category\n",
    "- Sort the data by category and month\n",
    "- Store the result in a new table in Snowflake "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Snowpark package\n",
    "\n",
    "Before we can begin, we must import the required package from the InterWorks Snowpark package and leverage it to create a Snowflake Snowpark Session object that is connected to our Snowflake environment. Alternatively, you can modify the code to establish a Snowflake Snowpark Session through any method of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Import module to build snowpark \n",
    "from interworks_snowpark.snowpark_session_builder import build_snowpark_session_via_parameters_json as build_snowpark_session\n",
    "\n",
    "## Generate Snowpark session\n",
    "snowpark_session = build_snowpark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve source table from staging\n",
    "\n",
    "Our source data is contained in the following object: `\"SALES_DB\".\"STAGING\".\"WEBSITE_SALES\"` and is read into a Snowflake dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = snowpark_session.table('\"SALES_DB\".\"STAGING\".\"WEBSITE_SALES\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be quickly queried to confirm the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "|\"HOUR_OF_OPERATION\"  |\"SALES\"  |\"WEBSITE_STATUS\"  |\"BOUNCE_RATE\"  |\n",
      "--------------------------------------------------------------------\n",
      "|2007-01-01 00:00:00  |4937.5   |STANDARD          |46             |\n",
      "|2007-01-01 01:00:00  |4752.1   |STANDARD          |46             |\n",
      "|2007-01-01 02:00:00  |4542.6   |STANDARD          |45             |\n",
      "|2007-01-01 03:00:00  |4357.7   |STANDARD          |45             |\n",
      "|2007-01-01 04:00:00  |4275.5   |STANDARD          |43             |\n",
      "|2007-01-01 05:00:00  |4274.7   |STANDARD          |39             |\n",
      "|2007-01-01 06:00:00  |4324.9   |STANDARD          |39             |\n",
      "|2007-01-01 07:00:00  |4350.0   |STANDARD          |43             |\n",
      "|2007-01-01 08:00:00  |4480.9   |STANDARD          |39             |\n",
      "|2007-01-01 09:00:00  |4664.2   |STANDARD          |45             |\n",
      "--------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_source.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "We are now ready to begin our transformations. Before we can do so, we must import a few functions from the `snowflake.snowpark.functions` module. Most notably, we wish to import `col` as this allows us to target a specific field in a Snowflake dataframe. In addition, we wish to import the native [Snowflake DATE_TRUNC function](https://docs.snowflake.com/en/sql-reference/functions/date_trunc.html) so that we can reduce a timestamp to a year/month, and the native [Snowflake YEAR function](https://docs.snowflake.com/en/sql-reference/functions/year.html) so that we can quickly identify and filter by the year of a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.snowpark.functions import year\n",
    "from snowflake.snowpark.functions import date_trunc\n",
    "from snowflake.snowpark.functions import sum as sf_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the data to 2017 onwards and only when the website is operating normally\n",
    "\n",
    "We leverage the `YEAR` function in Snowflake to determine the year for each record, then filter for when this is greater than or equal to 2017. We also apply a filter for when the \"WEBSITE_STATUS\" field has the value 'STANDARD'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "|\"HOUR_OF_OPERATION\"  |\"SALES\"  |\"WEBSITE_STATUS\"  |\"BOUNCE_RATE\"  |\n",
      "--------------------------------------------------------------------\n",
      "|2017-01-01 00:00:00  |4937.5   |STANDARD          |46             |\n",
      "|2017-01-01 01:00:00  |4752.1   |STANDARD          |46             |\n",
      "|2017-01-01 02:00:00  |4542.6   |STANDARD          |45             |\n",
      "|2017-01-01 03:00:00  |4357.7   |STANDARD          |45             |\n",
      "|2017-01-01 04:00:00  |4275.5   |STANDARD          |43             |\n",
      "|2017-01-01 05:00:00  |4274.7   |STANDARD          |39             |\n",
      "|2017-01-01 06:00:00  |4324.9   |STANDARD          |39             |\n",
      "|2017-01-01 07:00:00  |4350.0   |STANDARD          |43             |\n",
      "|2017-01-01 08:00:00  |4480.9   |STANDARD          |39             |\n",
      "|2017-01-01 09:00:00  |4664.2   |STANDARD          |45             |\n",
      "--------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df_source.filter((year(col(\"HOUR_OF_OPERATION\")) >= 2017) & (col(\"WEBSITE_STATUS\") == 'STANDARD'))\n",
    "\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorise each hour based on the bounce rate for the website (i.e. the percentage of website viewers who did not put an item in the basket)\n",
    "\n",
    "We use simple buckets to categorise the bounce rate using the following rules:\n",
    "\n",
    "- \"LOW\" when less than 30%\n",
    "- \"STANDARD\" when between 30% and 50%\n",
    "- \"HIGH\" when above 50%\n",
    "\n",
    "Another way of thinking about this is as follows:\n",
    "\n",
    "\"LOW\" < 30% <= \"MEDIUM\" < 50% <= \"HIGH\"\n",
    "\n",
    "We will apply this calculation by defining and leveraging an anonymous User Defined Function (UDF), for which we must first import the `udf` function so that we can define UDFs, and the `IntegerType` and `StringType` data types so we can define the data types that our UDF will input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required objects\n",
    "from snowflake.snowpark.types import IntegerType\n",
    "from snowflake.snowpark.types import StringType\n",
    "\n",
    "# Define UDF\n",
    "def categorise_bounce_rate(bounce_rate: int) :\n",
    "  if bounce_rate is None :\n",
    "    return None\n",
    "  elif bounce_rate < 30 :\n",
    "    return 'LOW'\n",
    "  elif bounce_rate < 50 :\n",
    "    return 'MEDIUM'\n",
    "  else :\n",
    "    return 'HIGH'\n",
    "\n",
    "# Create UDF\n",
    "sf_categorise_bounce_rate = snowpark_session.udf.register(\n",
    "    func = categorise_bounce_rate\n",
    "  , return_type = StringType()\n",
    "  , input_types = [IntegerType()]\n",
    "  , is_permanent = False\n",
    "  , name = '\"SALES_DB\".\"STAGING\".\"CATEGORISE_BOUNCE_RATE\"'\n",
    "  , replace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "|\"HOUR_OF_OPERATION\"  |\"SALES\"  |\"CATEGORY\"  |\n",
      "----------------------------------------------\n",
      "|2017-01-01 00:00:00  |4937.5   |MEDIUM      |\n",
      "|2017-01-01 01:00:00  |4752.1   |MEDIUM      |\n",
      "|2017-01-01 02:00:00  |4542.6   |MEDIUM      |\n",
      "|2017-01-01 03:00:00  |4357.7   |MEDIUM      |\n",
      "|2017-01-01 04:00:00  |4275.5   |MEDIUM      |\n",
      "|2017-01-01 05:00:00  |4274.7   |MEDIUM      |\n",
      "|2017-01-01 06:00:00  |4324.9   |MEDIUM      |\n",
      "|2017-01-01 07:00:00  |4350.0   |MEDIUM      |\n",
      "|2017-01-01 08:00:00  |4480.9   |MEDIUM      |\n",
      "|2017-01-01 09:00:00  |4664.2   |MEDIUM      |\n",
      "----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_categorised = df_filtered.select(col(\"HOUR_OF_OPERATION\"), col(\"SALES\"), sf_categorise_bounce_rate(col(\"BOUNCE_RATE\")).alias(\"CATEGORY\"))\n",
    "\n",
    "df_categorised.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate the number of sales by month and category\n",
    "\n",
    "Now that we have our categories, we are ready to group our data with the `groupby` method. Again, note how we leverage `sf_sum` to avoid using the standard Python `sum` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "|\"MONTH_OF_OPERATION\"  |\"CATEGORY\"  |\"SALES\"             |\n",
      "----------------------------------------------------------\n",
      "|2017-01-01 00:00:00   |LOW         |972043.5            |\n",
      "|2017-03-01 00:00:00   |LOW         |57151.5             |\n",
      "|2017-04-01 00:00:00   |MEDIUM      |1112902.9           |\n",
      "|2017-10-01 00:00:00   |MEDIUM      |196111.8            |\n",
      "|2018-04-01 00:00:00   |HIGH        |2340948.6           |\n",
      "|2018-09-01 00:00:00   |MEDIUM      |10138.099999999999  |\n",
      "|2019-02-01 00:00:00   |HIGH        |89087.7             |\n",
      "|2019-04-01 00:00:00   |HIGH        |2023355.1           |\n",
      "|2020-01-01 00:00:00   |LOW         |2060825.051         |\n",
      "|2020-03-01 00:00:00   |MEDIUM      |3182368.799         |\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_grouped = df_categorised \\\n",
    "  .group_by(date_trunc('month', col(\"HOUR_OF_OPERATION\")), col(\"CATEGORY\")) \\\n",
    "  .agg(sf_sum(col(\"SALES\"))) \\\n",
    "  .select(col(\"DATE_TRUNC(MONTH, HOUR_OF_OPERATION)\").alias(\"MONTH_OF_OPERATION\"), col(\"CATEGORY\"), col(\"SUM(SALES)\").alias(\"SALES\"))\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the data\n",
    "\n",
    "Using the `sort()` method, we can simply sort the data by category and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "|\"MONTH_OF_OPERATION\"  |\"CATEGORY\"  |\"SALES\"    |\n",
      "-------------------------------------------------\n",
      "|2017-01-01 00:00:00   |HIGH        |389788.9   |\n",
      "|2017-02-01 00:00:00   |HIGH        |361717.2   |\n",
      "|2017-03-01 00:00:00   |HIGH        |1956829.2  |\n",
      "|2017-04-01 00:00:00   |HIGH        |2752738.1  |\n",
      "|2017-05-01 00:00:00   |HIGH        |4338950.5  |\n",
      "|2017-06-01 00:00:00   |HIGH        |4776703.4  |\n",
      "|2017-07-01 00:00:00   |HIGH        |5880187.1  |\n",
      "|2017-08-01 00:00:00   |HIGH        |5671478.3  |\n",
      "|2017-09-01 00:00:00   |HIGH        |4602517.4  |\n",
      "|2017-10-01 00:00:00   |HIGH        |3830905.8  |\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df_grouped.sort(col(\"CATEGORY\"), col(\"MONTH_OF_OPERATION\"))\n",
    "\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the result in a new table in Snowflake\n",
    "\n",
    "Finally, we can output the data into a table in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.write.mode(\"overwrite\").save_as_table('\"SALES_DB\".\"CLEAN\".\"WEBSITE_SALES\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Results\n",
    "\n",
    "We can connect directly to our new table in Snowflake to verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "|\"MONTH_OF_OPERATION\"  |\"CATEGORY\"  |\"SALES\"    |\n",
      "-------------------------------------------------\n",
      "|2017-01-01 00:00:00   |HIGH        |389788.9   |\n",
      "|2017-02-01 00:00:00   |HIGH        |361717.2   |\n",
      "|2017-03-01 00:00:00   |HIGH        |1956829.2  |\n",
      "|2017-04-01 00:00:00   |HIGH        |2752738.1  |\n",
      "|2017-05-01 00:00:00   |HIGH        |4338950.5  |\n",
      "|2017-06-01 00:00:00   |HIGH        |4776703.4  |\n",
      "|2017-07-01 00:00:00   |HIGH        |5880187.1  |\n",
      "|2017-08-01 00:00:00   |HIGH        |5671478.3  |\n",
      "|2017-09-01 00:00:00   |HIGH        |4602517.4  |\n",
      "|2017-10-01 00:00:00   |HIGH        |3830905.8  |\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snowpark_session.table('\"SALES_DB\".\"CLEAN\".\"WEBSITE_SALES\"').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abca2f2ea4766feab0d5ed22ebf4aa677d5ebd9a43ee5e630cd7fa0b11fb4d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
